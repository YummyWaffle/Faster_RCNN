{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torchvision.ops import RoIPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>特征提取网络 - CNN的卷积层部分</font>\n",
    "\n",
    "<font size = 3>Faster RCNN在特征提取这块几乎没有什么限制，对图像大小也没有要求，CNN选择非常灵活 e.g. VGG / ResNet / DenseNet ...</font>\n",
    "\n",
    "<font size = 3>但一定要清楚CNN的降采样率(Feat_Stride)，此参数要用于锚框生成</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>RPN-Region Proposal Network</font>\n",
    "\n",
    " * <font size = 3> 目的 - 提取可能含有目标的区域，对应RCNN和Fast RCNN的Selective Search算法，只是用神经网络完成这个过程</font>\n",
    " * <font size = 3> 构成与流程</font>\n",
    "     - <font size = 2>RPN网络 - RPN Network (预测前景背景、与锚框回归参数) - 训练、测试</font>\n",
    "     \n",
    "     - <font size = 2>锚框生成层 - Anchor Generation Layer - 训练、测试</font>\n",
    "   \n",
    "     - <font size = 2>目标区提取层 - Proposal Layer - 训练、测试</font>\n",
    "   \n",
    "     - <font size = 2>锚框误差计算层 - Anchor Target Layer - 训练</font>\n",
    "   \n",
    "     - <font size = 2>目标区误差计算层 - Proposal Target Layer - 训练</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4>RPN - 网络 RPN Network</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2304, 2]) torch.Size([1, 2304, 4])\n"
     ]
    }
   ],
   "source": [
    "class rpn_net(nn.Module):\n",
    "    def __init__(self,inplanes=512,anchor_num=9):\n",
    "        super(rpn_net,self).__init__()\n",
    "        self.rpn_conv = nn.Conv2d(inplanes,512,3,1,1)\n",
    "        self.rpn_relu = nn.ReLU(inplace=True)\n",
    "        self.cls_conv = nn.Conv2d(512,anchor_num*2,1,1,0)\n",
    "        self.reg_conv = nn.Conv2d(512,anchor_num*4,1,1,0)\n",
    "    def forward(self,x):\n",
    "        x = self.rpn_conv(x)\n",
    "        x = self.rpn_relu(x)\n",
    "        cls_pred = self.cls_conv(x)\n",
    "        reg_pred = self.reg_conv(x)\n",
    "        cls_pred = cls_pred.permute(0,2,3,1).contiguous().view(x.size(0),-1,2)\n",
    "        reg_pred = reg_pred.permute(0,2,3,1).contiguous().view(x.size(0),-1,4)\n",
    "        return cls_pred,reg_pred\n",
    "\n",
    "rpn_network = rpn_net().cuda()\n",
    "base_feat = torch.randn(size=(1,512,16,16)).cuda()\n",
    "cls_pred,reg_pred = rpn_network(base_feat)\n",
    "print(cls_pred.size(),reg_pred.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4>锚框生成 - Anchor Generation Layer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# From Center XY & WH Form\n",
    "def to_corner_form(anchors):\n",
    "    return torch.cat((anchors[:,:2]-0.5*anchors[:,2:],anchors[:,:2]+0.5*anchors[:,2:]),1)\n",
    "    \n",
    "# From Conrner Form   \n",
    "def to_xywh_form(anchors):\n",
    "    return torch.cat((anchors[:,:2]+0.5*(anchors[:,2:]-anchors[:,:2]),anchors[:,2:]-anchors[:,:2]),1)\n",
    "\n",
    "# Ratio Transform Equation\n",
    "# w' = sqrt(w*h/ratio) = sqrt(area/ratio)\n",
    "# h' = sqrt(w*h*ratio) = sqrt(area*ratio)\n",
    "def ratio_trans(anchors,ratios):\n",
    "    anchor_xywh=to_xywh_form(anchors)\n",
    "    area = anchor_xywh[:,2] * anchor_xywh[:,3]\n",
    "    ratio_anchors = []\n",
    "    for ratio in ratios:\n",
    "        w_ratio = torch.sqrt(area/ratio)\n",
    "        h_ratio = torch.sqrt(area*ratio)\n",
    "        ratio_anchor_temp = anchor_xywh.clone()\n",
    "        ratio_anchor_temp[:,2] = w_ratio\n",
    "        ratio_anchor_temp[:,3] = h_ratio\n",
    "        ratio_anchor_temp=ratio_anchor_temp.numpy()\n",
    "        ratio_anchors.extend(ratio_anchor_temp)\n",
    "        #torch.cat((ratio_anchors,ratio_anchor_temp),0)\n",
    "    ratio_anchors = torch.tensor(ratio_anchors)\n",
    "    return to_corner_form(ratio_anchors)\n",
    "\n",
    "# Scale Transform Equation\n",
    "# w' = w * scale\n",
    "# h' = h * scale\n",
    "def scale_trans(anchors,scales):\n",
    "    anchor_xywh=to_xywh_form(anchors)\n",
    "    scale_anchors = []\n",
    "    for scale in scales:\n",
    "        w_scale = anchor_xywh[:,2] * scale\n",
    "        h_scale = anchor_xywh[:,3] * scale\n",
    "        scale_anchor_temp = anchor_xywh.clone()\n",
    "        scale_anchor_temp[:,2] = w_scale\n",
    "        scale_anchor_temp[:,3] = h_scale\n",
    "        scale_anchor_temp = scale_anchor_temp.numpy()\n",
    "        scale_anchors.extend(scale_anchor_temp)\n",
    "    scale_anchors = torch.tensor(scale_anchors)\n",
    "    return to_corner_form(scale_anchors)\n",
    "    \n",
    "def AnchorGenerator(im_info=None,featmap_info=None,base_size=16,ratios=[0.5,1.,2.],scales=[8.,16.,32.]):\n",
    "    # Step 1: Generate Base Anchor \n",
    "    base_anchor = torch.tensor([500,500,500+base_size-1,500+base_size-1]).unsqueeze(0).float()\n",
    "    # Step 2: ratios transformation\n",
    "    ratio_anchors = ratio_trans(base_anchor,ratios)\n",
    "    # Step 3: scales transformation\n",
    "    scale_anchors = scale_trans(ratio_anchors,scales)\n",
    "    return scale_anchors\n",
    "\n",
    "anchors = AnchorGenerator(scales=[2.,8.,32.])\n",
    "#print(anchors)\n",
    "img = np.zeros((1000,1000,3),dtype=np.uint8)\n",
    "img.fill(255)\n",
    "color = [(0,0,255),(0,255,0),(255,0,0)]\n",
    "for i,anchor in enumerate(anchors):\n",
    "    x1 = int(anchor[0])\n",
    "    y1 = int(anchor[1])\n",
    "    x2 = int(anchor[2])\n",
    "    y2 = int(anchor[3])\n",
    "    color_index = math.ceil((i + 1)/3)-1\n",
    "    cv2.rectangle(img,(x1,y1),(x2,y2),color[color_index],3)\n",
    "\n",
    "img2 = img[:,:,::-1]\n",
    "plt.imshow(img2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def AnchorGenerator(im_info=None,featmap_info=None,base_size=16,ratios=[0.5,1.,2.],scales=[8.,16.,32.]):\n",
    "    \"\"\"\n",
    "        Params - im_info (height,width)\n",
    "                 featmap_info (height,width,channel,feat_stride)\n",
    "        \n",
    "    \"\"\"\n",
    "    # Step 1: Generate Base Anchor \n",
    "    base_anchor = to_corner_form(torch.tensor([0,0,base_size,base_size]).unsqueeze(0).float())\n",
    "    # Step 2: ratios transformation\n",
    "    ratio_anchors = ratio_trans(base_anchor,ratios)\n",
    "    # Step 3: scales transformation\n",
    "    scale_anchors = scale_trans(ratio_anchors,scales)\n",
    "    # Step 4: Lay Out On Origin Images\n",
    "    shift_x = np.arange(featmap_info[0]) * featmap_info[2]\n",
    "    shift_y = np.arange(featmap_info[1]) * featmap_info[2]\n",
    "    shift_x,shift_y = np.meshgrid(shift_x,shift_y)\n",
    "    shifts = torch.from_numpy(np.vstack((shift_x.ravel(),shift_y.ravel(),shift_x.ravel(),shift_y.ravel())).transpose())\n",
    "    A = len(ratios) * len(scales)\n",
    "    K = shifts.size(0)\n",
    "    # Broadcasting Machanism scale_anchors - (Anchor_Num,4)  shifts - (Featmap_Size,4)\n",
    "    # Expected Size - (Featmap_Size*A,4) \n",
    "    # Process - [Tips：Because The Parameters Are Permuted As (BatchSize,H,W,Params.) So The Temp Result Should Be (K,A,4)] \n",
    "    #(1,Anchor_Num,4) + (Featmap_Size,1,4) - (Featmap_Size,Anchor_Num,4) - (Anchor_Num,Featmap_Size,4)\n",
    "    anchors = scale_anchors.view(1,A,4) + shifts.view(K,1,4)\n",
    "    anchors = anchors.view(K*A,4)\n",
    "    anchors[0::2].clamp_(0,im_info[0]-1)\n",
    "    anchors[1::2].clamp_(0,im_info[1]-1)\n",
    "    return anchors\n",
    "\n",
    "anchors = AnchorGenerator((100,100),(10,10,10),ratios=[0.5,2.,1],scales=[2.,])\n",
    "img = np.zeros((100,100,3),dtype=np.uint8)\n",
    "img.fill(255)\n",
    "for i,anchor in enumerate(anchors):\n",
    "    x1 = int(anchor[0])\n",
    "    y1 = int(anchor[1])\n",
    "    x2 = int(anchor[2])\n",
    "    y2 = int(anchor[3])\n",
    "    color = (0,0,255)\n",
    "    line_size = 1\n",
    "    if(i>=165 and i<168):\n",
    "        cv2.rectangle(img,(x1,y1),(x2,y2),color,line_size)\n",
    "    if(i>=150 and i<153):\n",
    "        cv2.rectangle(img,(x1,y1),(x2,y2),(255,0,0),line_size)\n",
    "    if(i>=0 and i<3):\n",
    "        cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),line_size)\n",
    "    #cv2.rectangle(img,(x1,y1),(x2,y2),color,line_size)\n",
    "img2 = img[:,:,::-1]\n",
    "plt.imshow(img2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4>目标区提取层 - Proposal Layer</font>\n",
    "\n",
    "<font size=3>主要工作</font>\n",
    "  * <font size=3>进行边框回归</font>\n",
    "  * <font size=3>选取分数最高的M个区域作为ROI</font>\n",
    "  * <font size=3>NMS(Non-Maximum Suppression)-非极大值抑制</font>\n",
    "  * <font size=3>选取分数最高的N个区域作为ROI</font>\n",
    " \n",
    "<font size=3>边框回归公式</font>\n",
    " $$x^{'}=x_{box}+t_{x}\\times w_{box}$$\n",
    " $$y^{'}=y_{box}+t_{y}\\times h_{box}$$\n",
    " $$w^{'}=e^{t_{w}}\\times w_{box}$$\n",
    " $$h^{'}=e^{t_{h}}\\times h_{box}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>nms module</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes,score,thr=0.7):\n",
    "    _,order = torch.sort(score,descending=True)\n",
    "    area = (boxes[:,2]-boxes[:,0]+1)*(boxes[:,3]-boxes[:,1]+1)\n",
    "    keep_indexes = []\n",
    "    while order.size(0)!= 0:\n",
    "        keep_index = order[0].item()\n",
    "        keep_indexes.append(keep_index)\n",
    "        order = order[1:]\n",
    "        xx1 = torch.max(boxes[keep_index,0],boxes[order[:],0])\n",
    "        yy1 = torch.max(boxes[keep_index,1],boxes[order[:],1])\n",
    "        xx2 = torch.min(boxes[keep_index,2],boxes[order[:],2])\n",
    "        yy2 = torch.min(boxes[keep_index,3],boxes[order[:],3])\n",
    "        w = (xx2-xx1+1).clamp_(min=0.)\n",
    "        h = (yy2-yy1+1).clamp_(min=0.)\n",
    "        inter = w * h\n",
    "        iou = inter / (area[keep_index] + area[order[:]] - inter)\n",
    "        not_same_indxes = iou[:] <= thr\n",
    "        order = order[not_same_indxes]\n",
    "    return torch.LongTensor(keep_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img1 = np.zeros((200,200,3),np.uint8)\n",
    "img1.fill(255)\n",
    "color = [(0,0,255),(0,255,0),(255,0,0),(255,255,0)] \n",
    "boxes = torch.tensor([[10,10,120,120],[1,1,101,101],[25,25,60,60],[50,50,170,170]]).float()\n",
    "scores = torch.tensor([0.6,0.9,0.5,0.7])\n",
    "for i,box in enumerate(boxes):\n",
    "    x1=int(box[0])\n",
    "    y1=int(box[1])\n",
    "    x2=int(box[2])\n",
    "    y2=int(box[3])\n",
    "    cv2.rectangle(img1,(x1,y1),(x2,y2),color[i],2)\n",
    "img1 = img1[:,:,::-1]\n",
    "plt.imshow(img1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "keep = nms(boxes,scores,0.6)\n",
    "print(keep)\n",
    "boxes = boxes[keep]\n",
    "color_keep = []\n",
    "for keep_ind in keep:\n",
    "    keep_ind = int(keep_ind)\n",
    "    color_keep.append(color[keep_ind])\n",
    "img1 = np.zeros((200,200,3),np.uint8)\n",
    "img1.fill(255)\n",
    "for i,box in enumerate(boxes):\n",
    "    x1=int(box[0])\n",
    "    y1=int(box[1])\n",
    "    x2=int(box[2])\n",
    "    y2=int(box[3])\n",
    "    cv2.rectangle(img1,(x1,y1),(x2,y2),color_keep[i],2)\n",
    "img1 = img1[:,:,::-1]\n",
    "plt.imshow(img1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4>锚框误差计算层 - Anchor Target Layer</font>\n",
    "  * <font size = 3>计算正负样本 - 通过锚框与标签的交并比计算，大于正样本阈值及标签最大交并比锚框为正样本，小于负样本阈值为负样本，其余样本成为'灰色样本'，不参与到计算中，可以忽略，同时注意该处正负样本比一般为 1 : 1 </font>\n",
    "  * <font size = 3>分类误差 - Cross_Entropy</font>\n",
    "  * <font size = 3>定位误差 - Smooth L1 Loss</font>\n",
    "  \n",
    "<font size = 3 >为何使用Smooth L1 Loss?</font>\n",
    "\n",
    "<font size = 3 >l1 loss and it's derivative</font>\n",
    "\n",
    "$$L_1 = |f(x)-Y|$$\n",
    "$$L_1'= \\pm f'(x)$$\n",
    "\n",
    "\n",
    "<font size = 3 >smotth l1 loss and it's derivative</font>\n",
    "$$\n",
    "Smooth L1=\\left\\{\\begin{array}{11}\n",
    "0.5x^2 & |x|<1 \\\\\n",
    "|x|-0.5 & x<1  or  x>1\n",
    "\\end{array}\\right.\\\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "Smooth L1'=\\left\\{\\begin{array}{11}\n",
    "x & |x|<1 \\\\\n",
    "-1 & x<-1\\\\\n",
    "1 & x>1\n",
    "\\end{array}\\right.\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4>目标区误差计算层 - Proposal Target Layer</font>\n",
    "  * <font size = 3>计算正负样本 - 通过锚框与标签的交并比计算，大于正样本阈值及标签最大交并比锚框为正样本，小于负样本阈值为负样本，并附上真是类别，其余样本成为'灰色样本'，不参与到计算中，可以忽略，同时注意该处正负样本比一般为 1 : 3 </font>\n",
    "  * <font size = 3>分类误差 - Cross_Entropy</font>\n",
    "  * <font size = 3>定位误差 - Smooth L1 Loss</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>ROI-Pooling</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1., 1., 3., 3., 3., 3.],\n",
      "          [1., 1., 1., 1., 3., 3., 3., 3.],\n",
      "          [1., 1., 1., 1., 3., 3., 3., 3.],\n",
      "          [1., 1., 1., 1., 3., 3., 3., 3.],\n",
      "          [2., 2., 2., 2., 4., 4., 4., 4.],\n",
      "          [2., 2., 2., 2., 4., 4., 4., 4.],\n",
      "          [2., 2., 2., 2., 4., 4., 4., 4.],\n",
      "          [2., 2., 2., 2., 4., 4., 4., 4.]]]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(size=(4,4))\n",
    "part1 = ones * 1\n",
    "part2 = ones * 2\n",
    "part3 = ones * 3\n",
    "part4 = ones * 4\n",
    "up_block = torch.cat((part1,part2),0)\n",
    "down_block = torch.cat((part3,part4),0)\n",
    "pic = torch.cat((up_block,down_block),1)\n",
    "pic = pic.unsqueeze(0).unsqueeze(0)\n",
    "print(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 3.],\n",
      "          [2., 4.]]]])\n"
     ]
    }
   ],
   "source": [
    "pooler = RoIPool(2,0.5)\n",
    "box = torch.tensor([[0.,0.,0.,14.,14.]])\n",
    "pool_result = pooler(pic,box)\n",
    "print(pool_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>分类回归网络</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 3., 2., 4.]])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 21]) torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "class cls_reg_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cls_reg_net,self).__init__()\n",
    "        self.fully_connected=nn.Linear(2*2*1,2048,bias=True)\n",
    "        self.classifier = nn.Linear(2048,21)\n",
    "        #self.regressor = nn.Linear(2048,21*4)\n",
    "        self.regressor = nn.Linear(2048,4)\n",
    "    def forward(self,x):\n",
    "        deeper_feat = self.fully_connected(x)\n",
    "        cls_pred = self.classifier(deeper_feat)\n",
    "        reg_pred = self.regressor(deeper_feat)\n",
    "        return cls_pred,reg_pred\n",
    "pool_result = pool_result.view(pool_result.size(0),-1)\n",
    "print(pool_result)\n",
    "print(pool_result.size())\n",
    "model = cls_reg_net()\n",
    "cls_pred,reg_pred = model(pool_result)\n",
    "print(cls_pred.size(),reg_pred.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>复现结果</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Epoch: 0 Batch: 0  Loss: 7.032528877258301\n",
      "Epoch: 0 Batch: 1  Loss: 23.61583137512207\n",
      "Epoch: 0 Batch: 2  Loss: 10.689197540283203\n",
      "Epoch: 0 Batch: 3  Loss: 33.90428924560547\n",
      "Epoch: 0 Batch: 4  Loss: 11.683389663696289\n",
      "Epoch: 0 Batch: 5  Loss: 33.34959411621094\n",
      "Epoch: 0 Batch: 6  Loss: 8.0436372756958\n",
      "Epoch: 0 Batch: 7  Loss: 9.20599365234375\n",
      "Epoch: 0 Batch: 8  Loss: 26.21234130859375\n",
      "Epoch: 0 Batch: 9  Loss: 53.535770416259766\n",
      "Epoch: 0 Batch: 10  Loss: 18.25165367126465\n",
      "Epoch: 0 Batch: 11  Loss: 24.272615432739258\n",
      "Epoch: 0 Batch: 12  Loss: 11.886892318725586\n",
      "Epoch: 0 Batch: 13  Loss: 4.050462245941162\n",
      "Epoch: 0 Batch: 14  Loss: 11.332605361938477\n",
      "Epoch: 0 Batch: 15  Loss: 13.210806846618652\n",
      "Epoch: 0 Batch: 16  Loss: 7.812825679779053\n",
      "Epoch: 0 Batch: 17  Loss: 11.133386611938477\n",
      "Epoch: 0 Batch: 18  Loss: 11.958693504333496\n",
      "Epoch: 0 Batch: 19  Loss: 4.253428936004639\n",
      "Epoch: 0 Batch: 20  Loss: 4.634396553039551\n",
      "Epoch: 0 Batch: 21  Loss: 4.914981365203857\n",
      "Epoch: 0 Batch: 22  Loss: 4.3313493728637695\n",
      "Epoch: 0 Batch: 23  Loss: 4.564672946929932\n",
      "Epoch: 0 Batch: 24  Loss: 2.929595708847046\n",
      "Epoch: 0 Batch: 25  Loss: 3.5307371616363525\n",
      "Epoch: 0 Batch: 26  Loss: 2.871389627456665\n",
      "Epoch: 0 Batch: 27  Loss: 18.526695251464844\n",
      "Epoch: 0 Batch: 28  Loss: 21.527498245239258\n",
      "Epoch: 0 Batch: 29  Loss: 6.169014930725098\n",
      "Epoch: 0 Batch: 30  Loss: 8.194326400756836\n",
      "Epoch: 0 Batch: 31  Loss: 4.101970195770264\n",
      "Epoch: 0 Batch: 32  Loss: 13.458154678344727\n",
      "Epoch: 0 Batch: 33  Loss: 8.168590545654297\n",
      "Epoch: 0 Batch: 34  Loss: 2.580143928527832\n",
      "Epoch: 0 Batch: 35  Loss: 9.827560424804688\n",
      "Epoch: 0 Batch: 36  Loss: 2.307295322418213\n",
      "Epoch: 0 Batch: 37  Loss: 9.729652404785156\n",
      "Epoch: 0 Batch: 38  Loss: 2.2872369289398193\n",
      "Epoch: 0 Batch: 39  Loss: 15.839043617248535\n",
      "Epoch: 0 Batch: 40  Loss: 4.214990139007568\n",
      "Epoch: 0 Batch: 41  Loss: 4.912476539611816\n",
      "Epoch: 0 Batch: 42  Loss: 2.044339656829834\n",
      "Epoch: 0 Batch: 43  Loss: 3.9747259616851807\n",
      "Epoch: 0 Batch: 44  Loss: 2.0666308403015137\n",
      "Epoch: 0 Batch: 45  Loss: 3.537069320678711\n",
      "Epoch: 0 Batch: 46  Loss: 2.9667036533355713\n",
      "Epoch: 0 Batch: 47  Loss: 3.034712076187134\n",
      "Epoch: 0 Batch: 48  Loss: 14.848814964294434\n",
      "Epoch: 0 Batch: 49  Loss: 2.9601633548736572\n",
      "Epoch: 0 Batch: 50  Loss: 2.2713463306427\n",
      "Epoch: 0 Batch: 51  Loss: 4.132848262786865\n",
      "Epoch: 0 Batch: 52  Loss: 1.9878398180007935\n",
      "Epoch: 0 Batch: 53  Loss: 3.87385892868042\n",
      "Epoch: 0 Batch: 54  Loss: 2.18581223487854\n",
      "Epoch: 0 Batch: 55  Loss: 2.0544166564941406\n",
      "Epoch: 0 Batch: 56  Loss: 1.8377532958984375\n",
      "Epoch: 0 Batch: 57  Loss: 2.9981274604797363\n",
      "Epoch: 0 Batch: 58  Loss: 3.2866110801696777\n",
      "Epoch: 0 Batch: 59  Loss: 2.292733669281006\n",
      "Epoch: 0 Batch: 60  Loss: 8.469878196716309\n",
      "Epoch: 0 Batch: 61  Loss: 4.224017143249512\n",
      "Epoch: 0 Batch: 62  Loss: 2.177614450454712\n",
      "Epoch: 0 Batch: 63  Loss: 2.8915257453918457\n",
      "Epoch: 0 Batch: 64  Loss: 3.9531149864196777\n",
      "Epoch: 0 Batch: 65  Loss: 4.336031436920166\n",
      "Epoch: 0 Batch: 66  Loss: 3.057316541671753\n",
      "Epoch: 0 Batch: 67  Loss: 2.2943520545959473\n",
      "Epoch: 0 Batch: 68  Loss: 3.865537405014038\n",
      "Epoch: 0 Batch: 69  Loss: 12.029166221618652\n",
      "Epoch: 0 Batch: 70  Loss: 13.450994491577148\n",
      "Epoch: 0 Batch: 71  Loss: 4.796875476837158\n",
      "Epoch: 0 Batch: 72  Loss: 4.763581275939941\n",
      "Epoch: 0 Batch: 73  Loss: 2.1614201068878174\n",
      "Epoch: 0 Batch: 74  Loss: 5.096813678741455\n",
      "Epoch: 0 Batch: 75  Loss: 4.857375144958496\n",
      "Epoch: 0 Batch: 76  Loss: 3.174896240234375\n",
      "Epoch: 0 Batch: 77  Loss: 2.9214026927948\n",
      "Epoch: 0 Batch: 78  Loss: 8.860453605651855\n",
      "Epoch: 0 Batch: 79  Loss: 4.981077671051025\n",
      "------------------------------------------------\n",
      "Epoch: 1 Batch: 0  Loss: 3.7694597244262695\n",
      "Epoch: 1 Batch: 1  Loss: 2.5871944427490234\n",
      "Epoch: 1 Batch: 2  Loss: 4.607030868530273\n",
      "Epoch: 1 Batch: 3  Loss: 2.7734508514404297\n",
      "Epoch: 1 Batch: 4  Loss: 2.0984392166137695\n",
      "Epoch: 1 Batch: 5  Loss: 3.9809277057647705\n",
      "Epoch: 1 Batch: 6  Loss: 3.4210855960845947\n",
      "Epoch: 1 Batch: 7  Loss: 7.52232551574707\n",
      "Epoch: 1 Batch: 8  Loss: 3.2422688007354736\n",
      "Epoch: 1 Batch: 9  Loss: 2.2827982902526855\n",
      "Epoch: 1 Batch: 10  Loss: 1.6792652606964111\n",
      "Epoch: 1 Batch: 11  Loss: 4.164789199829102\n",
      "Epoch: 1 Batch: 12  Loss: 2.9771318435668945\n",
      "Epoch: 1 Batch: 13  Loss: 2.0149126052856445\n",
      "Epoch: 1 Batch: 14  Loss: 4.427688121795654\n",
      "Epoch: 1 Batch: 15  Loss: 2.4394736289978027\n",
      "Epoch: 1 Batch: 16  Loss: 8.245741844177246\n",
      "Epoch: 1 Batch: 17  Loss: 3.4615321159362793\n",
      "Epoch: 1 Batch: 18  Loss: 4.22075080871582\n",
      "Epoch: 1 Batch: 19  Loss: 15.79170036315918\n",
      "Epoch: 1 Batch: 20  Loss: 8.950586318969727\n",
      "Epoch: 1 Batch: 21  Loss: 4.949978351593018\n",
      "Epoch: 1 Batch: 22  Loss: 3.303621292114258\n",
      "Epoch: 1 Batch: 23  Loss: 2.151621103286743\n",
      "Epoch: 1 Batch: 24  Loss: 5.040050983428955\n",
      "Epoch: 1 Batch: 25  Loss: 4.442326545715332\n",
      "Epoch: 1 Batch: 26  Loss: 3.7794547080993652\n",
      "Epoch: 1 Batch: 27  Loss: 1.8835985660552979\n",
      "Epoch: 1 Batch: 28  Loss: 3.221041679382324\n",
      "Epoch: 1 Batch: 29  Loss: 11.902578353881836\n",
      "Epoch: 1 Batch: 30  Loss: 6.076773166656494\n",
      "Epoch: 1 Batch: 31  Loss: 4.036271572113037\n",
      "Epoch: 1 Batch: 32  Loss: 3.533151626586914\n",
      "Epoch: 1 Batch: 33  Loss: 5.024261474609375\n",
      "Epoch: 1 Batch: 34  Loss: 2.332117795944214\n",
      "Epoch: 1 Batch: 35  Loss: 5.503225803375244\n",
      "Epoch: 1 Batch: 36  Loss: 2.029456377029419\n",
      "Epoch: 1 Batch: 37  Loss: 1.8227179050445557\n",
      "Epoch: 1 Batch: 38  Loss: 2.6422178745269775\n",
      "Epoch: 1 Batch: 39  Loss: 7.4682393074035645\n",
      "Epoch: 1 Batch: 40  Loss: 3.706540822982788\n",
      "Epoch: 1 Batch: 41  Loss: 3.208538770675659\n",
      "Epoch: 1 Batch: 42  Loss: 3.445557117462158\n",
      "Epoch: 1 Batch: 43  Loss: 3.8274753093719482\n",
      "Epoch: 1 Batch: 44  Loss: 3.7366714477539062\n",
      "Epoch: 1 Batch: 45  Loss: 2.554706335067749\n",
      "Epoch: 1 Batch: 46  Loss: 3.0764052867889404\n",
      "Epoch: 1 Batch: 47  Loss: 4.4807233810424805\n",
      "Epoch: 1 Batch: 48  Loss: 2.8122363090515137\n",
      "Epoch: 1 Batch: 49  Loss: 3.103982925415039\n",
      "Epoch: 1 Batch: 50  Loss: 2.88706374168396\n",
      "Epoch: 1 Batch: 51  Loss: 2.5618174076080322\n",
      "Epoch: 1 Batch: 52  Loss: 2.0836312770843506\n",
      "Epoch: 1 Batch: 53  Loss: 2.7297496795654297\n",
      "Epoch: 1 Batch: 54  Loss: 2.5898633003234863\n",
      "Epoch: 1 Batch: 55  Loss: 1.8303433656692505\n",
      "Epoch: 1 Batch: 56  Loss: 2.4662225246429443\n",
      "Epoch: 1 Batch: 57  Loss: 7.564964771270752\n",
      "Epoch: 1 Batch: 58  Loss: 5.036555290222168\n",
      "Epoch: 1 Batch: 59  Loss: 3.024780035018921\n",
      "Epoch: 1 Batch: 60  Loss: 2.0895509719848633\n",
      "Epoch: 1 Batch: 61  Loss: 3.672318458557129\n",
      "Epoch: 1 Batch: 62  Loss: 2.664039134979248\n",
      "Epoch: 1 Batch: 63  Loss: 3.1367483139038086\n",
      "Epoch: 1 Batch: 64  Loss: 6.509272575378418\n",
      "Epoch: 1 Batch: 65  Loss: 7.832836151123047\n",
      "Epoch: 1 Batch: 66  Loss: 2.2247164249420166\n",
      "Epoch: 1 Batch: 67  Loss: 2.0193936824798584\n",
      "Epoch: 1 Batch: 68  Loss: 1.9345818758010864\n",
      "Epoch: 1 Batch: 69  Loss: 1.1794415712356567\n",
      "Epoch: 1 Batch: 70  Loss: 2.806847333908081\n",
      "Epoch: 1 Batch: 71  Loss: 4.786574840545654\n",
      "Epoch: 1 Batch: 72  Loss: 13.772107124328613\n",
      "Epoch: 1 Batch: 73  Loss: 2.1457674503326416\n",
      "Epoch: 1 Batch: 74  Loss: 3.6964914798736572\n",
      "Epoch: 1 Batch: 75  Loss: 2.1318447589874268\n",
      "Epoch: 1 Batch: 76  Loss: 2.4872472286224365\n",
      "Epoch: 1 Batch: 77  Loss: 2.059830904006958\n",
      "Epoch: 1 Batch: 78  Loss: 5.3224663734436035\n",
      "Epoch: 1 Batch: 79  Loss: 2.607931137084961\n",
      "------------------------------------------------\n",
      "Epoch: 2 Batch: 0  Loss: 5.906670093536377\n",
      "Epoch: 2 Batch: 1  Loss: 2.1506481170654297\n",
      "Epoch: 2 Batch: 2  Loss: 2.8946542739868164\n",
      "Epoch: 2 Batch: 3  Loss: 1.5343111753463745\n",
      "Epoch: 2 Batch: 4  Loss: 1.7363990545272827\n",
      "Epoch: 2 Batch: 5  Loss: 5.0109148025512695\n",
      "Epoch: 2 Batch: 6  Loss: 6.79463005065918\n",
      "Epoch: 2 Batch: 7  Loss: 11.80380916595459\n",
      "Epoch: 2 Batch: 8  Loss: 3.132185220718384\n",
      "Epoch: 2 Batch: 9  Loss: 2.743813991546631\n",
      "Epoch: 2 Batch: 10  Loss: 2.8923590183258057\n",
      "Epoch: 2 Batch: 11  Loss: 1.8607878684997559\n",
      "Epoch: 2 Batch: 12  Loss: 5.74432897567749\n",
      "Epoch: 2 Batch: 13  Loss: 2.1845390796661377\n",
      "Epoch: 2 Batch: 14  Loss: 1.4664995670318604\n",
      "Epoch: 2 Batch: 15  Loss: 3.457129716873169\n",
      "Epoch: 2 Batch: 16  Loss: 4.167646408081055\n",
      "Epoch: 2 Batch: 17  Loss: 3.2078566551208496\n",
      "Epoch: 2 Batch: 18  Loss: 3.3969783782958984\n",
      "Epoch: 2 Batch: 19  Loss: 2.8709635734558105\n",
      "Epoch: 2 Batch: 20  Loss: 2.0164954662323\n",
      "Epoch: 2 Batch: 21  Loss: 6.197004318237305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Batch: 22  Loss: 2.112191915512085\n",
      "Epoch: 2 Batch: 23  Loss: 2.126471996307373\n",
      "Epoch: 2 Batch: 24  Loss: 1.8824305534362793\n",
      "Epoch: 2 Batch: 25  Loss: 1.7301392555236816\n",
      "Epoch: 2 Batch: 26  Loss: 2.3930301666259766\n",
      "Epoch: 2 Batch: 27  Loss: 2.32576060295105\n",
      "Epoch: 2 Batch: 28  Loss: 2.4968409538269043\n",
      "Epoch: 2 Batch: 29  Loss: 2.1334593296051025\n",
      "Epoch: 2 Batch: 30  Loss: 1.4630095958709717\n",
      "Epoch: 2 Batch: 31  Loss: 13.894343376159668\n",
      "Epoch: 2 Batch: 32  Loss: 7.042233467102051\n",
      "Epoch: 2 Batch: 33  Loss: 1.6590697765350342\n",
      "Epoch: 2 Batch: 34  Loss: 8.991294860839844\n",
      "Epoch: 2 Batch: 35  Loss: 3.787039041519165\n",
      "Epoch: 2 Batch: 36  Loss: 2.994866132736206\n",
      "Epoch: 2 Batch: 37  Loss: 1.941251277923584\n",
      "Epoch: 2 Batch: 38  Loss: 6.2515950202941895\n",
      "Epoch: 2 Batch: 39  Loss: 2.4363813400268555\n",
      "Epoch: 2 Batch: 40  Loss: 2.58685040473938\n",
      "Epoch: 2 Batch: 41  Loss: 2.0138180255889893\n",
      "Epoch: 2 Batch: 42  Loss: 2.3612325191497803\n",
      "Epoch: 2 Batch: 43  Loss: 2.4365622997283936\n",
      "Epoch: 2 Batch: 44  Loss: 2.2261626720428467\n",
      "Epoch: 2 Batch: 45  Loss: 5.66691255569458\n",
      "Epoch: 2 Batch: 46  Loss: 2.7826573848724365\n",
      "Epoch: 2 Batch: 47  Loss: 2.411238431930542\n",
      "Epoch: 2 Batch: 48  Loss: 4.724188804626465\n",
      "Epoch: 2 Batch: 49  Loss: 6.4153947830200195\n",
      "Epoch: 2 Batch: 50  Loss: 4.611873149871826\n",
      "Epoch: 2 Batch: 51  Loss: 2.3859848976135254\n",
      "Epoch: 2 Batch: 52  Loss: 1.5848610401153564\n",
      "Epoch: 2 Batch: 53  Loss: 2.9624183177948\n",
      "Epoch: 2 Batch: 54  Loss: 1.6222479343414307\n",
      "Epoch: 2 Batch: 55  Loss: 1.5601874589920044\n",
      "Epoch: 2 Batch: 56  Loss: 1.0925917625427246\n",
      "Epoch: 2 Batch: 57  Loss: 6.061248779296875\n",
      "Epoch: 2 Batch: 58  Loss: 2.0766751766204834\n",
      "Epoch: 2 Batch: 59  Loss: 9.159082412719727\n",
      "Epoch: 2 Batch: 60  Loss: 1.8592361211776733\n",
      "Epoch: 2 Batch: 61  Loss: 1.8311326503753662\n",
      "Epoch: 2 Batch: 62  Loss: 1.2689199447631836\n",
      "Epoch: 2 Batch: 63  Loss: 6.845628261566162\n",
      "Epoch: 2 Batch: 64  Loss: 4.744454383850098\n",
      "Epoch: 2 Batch: 65  Loss: 2.475741386413574\n",
      "Epoch: 2 Batch: 66  Loss: 5.570656776428223\n",
      "Epoch: 2 Batch: 67  Loss: 2.324382781982422\n",
      "Epoch: 2 Batch: 68  Loss: 7.660802841186523\n",
      "Epoch: 2 Batch: 69  Loss: 2.9819538593292236\n",
      "Epoch: 2 Batch: 70  Loss: 2.144896984100342\n",
      "Epoch: 2 Batch: 71  Loss: 2.802652359008789\n",
      "Epoch: 2 Batch: 72  Loss: 1.49398934841156\n",
      "Epoch: 2 Batch: 73  Loss: 2.2455666065216064\n",
      "Epoch: 2 Batch: 74  Loss: 1.7901601791381836\n",
      "Epoch: 2 Batch: 75  Loss: 1.2934907674789429\n",
      "Epoch: 2 Batch: 76  Loss: 3.3253042697906494\n",
      "Epoch: 2 Batch: 77  Loss: 2.020252227783203\n",
      "Epoch: 2 Batch: 78  Loss: 1.5812022686004639\n",
      "Epoch: 2 Batch: 79  Loss: 2.319514751434326\n",
      "------------------------------------------------\n",
      "Epoch: 3 Batch: 0  Loss: 1.7985020875930786\n",
      "Epoch: 3 Batch: 1  Loss: 6.182323932647705\n",
      "Epoch: 3 Batch: 2  Loss: 1.2522767782211304\n",
      "Epoch: 3 Batch: 3  Loss: 4.309160232543945\n",
      "Epoch: 3 Batch: 4  Loss: 1.1624822616577148\n",
      "Epoch: 3 Batch: 5  Loss: 1.1841593980789185\n",
      "Epoch: 3 Batch: 6  Loss: 1.5338040590286255\n",
      "Epoch: 3 Batch: 7  Loss: 1.6805444955825806\n",
      "Epoch: 3 Batch: 8  Loss: 2.126636505126953\n",
      "Epoch: 3 Batch: 9  Loss: 1.085364818572998\n",
      "Epoch: 3 Batch: 10  Loss: 1.0824358463287354\n",
      "Epoch: 3 Batch: 11  Loss: 0.9937171339988708\n",
      "Epoch: 3 Batch: 12  Loss: 1.4639499187469482\n",
      "Epoch: 3 Batch: 13  Loss: 5.7641987800598145\n",
      "Epoch: 3 Batch: 14  Loss: 2.9625682830810547\n",
      "Epoch: 3 Batch: 15  Loss: 1.5747647285461426\n",
      "Epoch: 3 Batch: 16  Loss: 1.9012572765350342\n",
      "Epoch: 3 Batch: 17  Loss: 2.2258875370025635\n",
      "Epoch: 3 Batch: 18  Loss: 0.9732075333595276\n",
      "Epoch: 3 Batch: 19  Loss: 1.3258557319641113\n",
      "Epoch: 3 Batch: 20  Loss: 0.9069610834121704\n",
      "Epoch: 3 Batch: 21  Loss: 1.9172141551971436\n",
      "Epoch: 3 Batch: 22  Loss: 5.1877121925354\n",
      "Epoch: 3 Batch: 23  Loss: 2.0455164909362793\n",
      "Epoch: 3 Batch: 24  Loss: 4.97202730178833\n",
      "Epoch: 3 Batch: 25  Loss: 1.7452692985534668\n",
      "Epoch: 3 Batch: 26  Loss: 3.2578859329223633\n",
      "Epoch: 3 Batch: 27  Loss: 1.3783334493637085\n",
      "Epoch: 3 Batch: 28  Loss: 3.7020647525787354\n",
      "Epoch: 3 Batch: 29  Loss: 7.3803324699401855\n",
      "Epoch: 3 Batch: 30  Loss: 2.145663261413574\n",
      "Epoch: 3 Batch: 31  Loss: 1.1702899932861328\n",
      "Epoch: 3 Batch: 32  Loss: 3.474708080291748\n",
      "Epoch: 3 Batch: 33  Loss: 1.6151140928268433\n",
      "Epoch: 3 Batch: 34  Loss: 1.0138792991638184\n",
      "Epoch: 3 Batch: 35  Loss: 2.023174524307251\n",
      "Epoch: 3 Batch: 36  Loss: 1.568672776222229\n",
      "Epoch: 3 Batch: 37  Loss: 1.8524391651153564\n",
      "Epoch: 3 Batch: 38  Loss: 9.760758399963379\n",
      "Epoch: 3 Batch: 39  Loss: 1.8604536056518555\n",
      "Epoch: 3 Batch: 40  Loss: 1.1291743516921997\n",
      "Epoch: 3 Batch: 41  Loss: 3.4213521480560303\n",
      "Epoch: 3 Batch: 42  Loss: 2.517481803894043\n",
      "Epoch: 3 Batch: 43  Loss: 1.8196769952774048\n",
      "Epoch: 3 Batch: 44  Loss: 2.1958320140838623\n",
      "Epoch: 3 Batch: 45  Loss: 2.0215044021606445\n",
      "Epoch: 3 Batch: 46  Loss: 1.677301049232483\n",
      "Epoch: 3 Batch: 47  Loss: 1.0686036348342896\n",
      "Epoch: 3 Batch: 48  Loss: 1.4581677913665771\n",
      "Epoch: 3 Batch: 49  Loss: 1.9177770614624023\n",
      "Epoch: 3 Batch: 50  Loss: 1.4215552806854248\n",
      "Epoch: 3 Batch: 51  Loss: 1.4827402830123901\n",
      "Epoch: 3 Batch: 52  Loss: 1.0478416681289673\n",
      "Epoch: 3 Batch: 53  Loss: 1.643372893333435\n",
      "Epoch: 3 Batch: 54  Loss: 11.562768936157227\n",
      "Epoch: 3 Batch: 55  Loss: 0.9614052176475525\n",
      "Epoch: 3 Batch: 56  Loss: 10.38533878326416\n",
      "Epoch: 3 Batch: 57  Loss: 1.8521816730499268\n",
      "Epoch: 3 Batch: 58  Loss: 1.1941633224487305\n",
      "Epoch: 3 Batch: 59  Loss: 1.6105806827545166\n",
      "Epoch: 3 Batch: 60  Loss: 2.3836846351623535\n",
      "Epoch: 3 Batch: 61  Loss: 1.2506399154663086\n",
      "Epoch: 3 Batch: 62  Loss: 3.105184555053711\n",
      "Epoch: 3 Batch: 63  Loss: 1.6821274757385254\n",
      "Epoch: 3 Batch: 64  Loss: 7.8465142250061035\n",
      "Epoch: 3 Batch: 65  Loss: 3.663794994354248\n",
      "Epoch: 3 Batch: 66  Loss: 1.557654857635498\n",
      "Epoch: 3 Batch: 67  Loss: 1.1195261478424072\n",
      "Epoch: 3 Batch: 68  Loss: 2.348731756210327\n",
      "Epoch: 3 Batch: 69  Loss: 0.6781188249588013\n",
      "Epoch: 3 Batch: 70  Loss: 1.2408185005187988\n",
      "Epoch: 3 Batch: 71  Loss: 6.159131050109863\n",
      "Epoch: 3 Batch: 72  Loss: 7.382328510284424\n",
      "Epoch: 3 Batch: 73  Loss: 2.7191543579101562\n",
      "Epoch: 3 Batch: 74  Loss: 1.0883876085281372\n",
      "Epoch: 3 Batch: 75  Loss: 3.780703067779541\n",
      "Epoch: 3 Batch: 76  Loss: 3.123256206512451\n",
      "Epoch: 3 Batch: 77  Loss: 2.031278371810913\n",
      "Epoch: 3 Batch: 78  Loss: 4.3749189376831055\n",
      "Epoch: 3 Batch: 79  Loss: 2.8744735717773438\n",
      "------------------------------------------------\n",
      "Epoch: 4 Batch: 0  Loss: 2.4662036895751953\n",
      "Epoch: 4 Batch: 1  Loss: 2.173128604888916\n",
      "Epoch: 4 Batch: 2  Loss: 1.0937128067016602\n",
      "Epoch: 4 Batch: 3  Loss: 7.736885070800781\n",
      "Epoch: 4 Batch: 4  Loss: 3.1552608013153076\n",
      "Epoch: 4 Batch: 5  Loss: 1.9226797819137573\n",
      "Epoch: 4 Batch: 6  Loss: 1.5984560251235962\n",
      "Epoch: 4 Batch: 7  Loss: 1.2910633087158203\n",
      "Epoch: 4 Batch: 8  Loss: 8.358288764953613\n",
      "Epoch: 4 Batch: 9  Loss: 2.8532044887542725\n",
      "Epoch: 4 Batch: 10  Loss: 3.116499185562134\n",
      "Epoch: 4 Batch: 11  Loss: 5.768142223358154\n",
      "Epoch: 4 Batch: 12  Loss: 1.5452115535736084\n",
      "Epoch: 4 Batch: 13  Loss: 1.38580322265625\n",
      "Epoch: 4 Batch: 14  Loss: 2.3201828002929688\n",
      "Epoch: 4 Batch: 15  Loss: 1.6161566972732544\n",
      "Epoch: 4 Batch: 16  Loss: 1.7031499147415161\n",
      "Epoch: 4 Batch: 17  Loss: 2.395415782928467\n",
      "Epoch: 4 Batch: 18  Loss: 2.2482190132141113\n",
      "Epoch: 4 Batch: 19  Loss: 1.4662402868270874\n",
      "Epoch: 4 Batch: 20  Loss: 1.2397141456604004\n",
      "Epoch: 4 Batch: 21  Loss: 0.8282918334007263\n",
      "Epoch: 4 Batch: 22  Loss: 1.8139467239379883\n",
      "Epoch: 4 Batch: 23  Loss: 1.1504623889923096\n",
      "Epoch: 4 Batch: 24  Loss: 3.0800840854644775\n",
      "Epoch: 4 Batch: 25  Loss: 9.726422309875488\n",
      "Epoch: 4 Batch: 26  Loss: 5.867593288421631\n",
      "Epoch: 4 Batch: 27  Loss: 2.6239218711853027\n",
      "Epoch: 4 Batch: 28  Loss: 1.8585833311080933\n",
      "Epoch: 4 Batch: 29  Loss: 1.5469987392425537\n",
      "Epoch: 4 Batch: 30  Loss: 8.21651840209961\n",
      "Epoch: 4 Batch: 31  Loss: 1.8688563108444214\n",
      "Epoch: 4 Batch: 32  Loss: 2.8597497940063477\n",
      "Epoch: 4 Batch: 33  Loss: 1.4556925296783447\n",
      "Epoch: 4 Batch: 34  Loss: 5.224978923797607\n",
      "Epoch: 4 Batch: 35  Loss: 1.3358728885650635\n",
      "Epoch: 4 Batch: 36  Loss: 1.5937696695327759\n",
      "Epoch: 4 Batch: 37  Loss: 1.142051100730896\n",
      "Epoch: 4 Batch: 38  Loss: 1.9590083360671997\n",
      "Epoch: 4 Batch: 39  Loss: 1.849083423614502\n",
      "Epoch: 4 Batch: 40  Loss: 1.52255117893219\n",
      "Epoch: 4 Batch: 41  Loss: 1.9108518362045288\n",
      "Epoch: 4 Batch: 42  Loss: 2.0188262462615967\n",
      "Epoch: 4 Batch: 43  Loss: 1.8097469806671143\n",
      "Epoch: 4 Batch: 44  Loss: 1.306200385093689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Batch: 45  Loss: 2.801487922668457\n",
      "Epoch: 4 Batch: 46  Loss: 1.131258249282837\n",
      "Epoch: 4 Batch: 47  Loss: 1.162063717842102\n",
      "Epoch: 4 Batch: 48  Loss: 1.0942192077636719\n",
      "Epoch: 4 Batch: 49  Loss: 1.5744962692260742\n",
      "Epoch: 4 Batch: 50  Loss: 1.279024362564087\n",
      "Epoch: 4 Batch: 51  Loss: 1.6240339279174805\n",
      "Epoch: 4 Batch: 52  Loss: 1.0565701723098755\n",
      "Epoch: 4 Batch: 53  Loss: 1.6640393733978271\n",
      "Epoch: 4 Batch: 54  Loss: 1.5113866329193115\n",
      "Epoch: 4 Batch: 55  Loss: 0.8614221811294556\n",
      "Epoch: 4 Batch: 56  Loss: 1.2059669494628906\n",
      "Epoch: 4 Batch: 57  Loss: 1.139661431312561\n",
      "Epoch: 4 Batch: 58  Loss: 2.113154172897339\n",
      "Epoch: 4 Batch: 59  Loss: 1.4748541116714478\n",
      "Epoch: 4 Batch: 60  Loss: 1.4472464323043823\n",
      "Epoch: 4 Batch: 61  Loss: 4.4044108390808105\n",
      "Epoch: 4 Batch: 62  Loss: 1.1235440969467163\n",
      "Epoch: 4 Batch: 63  Loss: 4.1108078956604\n",
      "Epoch: 4 Batch: 64  Loss: 1.9793026447296143\n",
      "Epoch: 4 Batch: 65  Loss: 0.9290292859077454\n",
      "Epoch: 4 Batch: 66  Loss: 1.1897621154785156\n",
      "Epoch: 4 Batch: 67  Loss: 6.653353691101074\n",
      "Epoch: 4 Batch: 68  Loss: 2.545245885848999\n",
      "Epoch: 4 Batch: 69  Loss: 2.2036328315734863\n",
      "Epoch: 4 Batch: 70  Loss: 1.1162656545639038\n",
      "Epoch: 4 Batch: 71  Loss: 3.2179274559020996\n",
      "Epoch: 4 Batch: 72  Loss: 7.362155914306641\n",
      "Epoch: 4 Batch: 73  Loss: 1.7461858987808228\n",
      "Epoch: 4 Batch: 74  Loss: 1.5978171825408936\n",
      "Epoch: 4 Batch: 75  Loss: 1.5068352222442627\n",
      "Epoch: 4 Batch: 76  Loss: 1.7347817420959473\n",
      "Epoch: 4 Batch: 77  Loss: 1.2219222784042358\n",
      "Epoch: 4 Batch: 78  Loss: 1.7962839603424072\n",
      "Epoch: 4 Batch: 79  Loss: 1.8494858741760254\n",
      "------------------------------------------------\n",
      "Epoch: 5 Batch: 0  Loss: 1.7105059623718262\n",
      "Epoch: 5 Batch: 1  Loss: 3.2799129486083984\n",
      "Epoch: 5 Batch: 2  Loss: 4.084504127502441\n",
      "Epoch: 5 Batch: 3  Loss: 1.684262990951538\n",
      "Epoch: 5 Batch: 4  Loss: 6.539182662963867\n",
      "Epoch: 5 Batch: 5  Loss: 1.7568212747573853\n",
      "Epoch: 5 Batch: 6  Loss: 6.260939121246338\n",
      "Epoch: 5 Batch: 7  Loss: 2.0255343914031982\n",
      "Epoch: 5 Batch: 8  Loss: 3.893096446990967\n",
      "Epoch: 5 Batch: 9  Loss: 1.880707025527954\n",
      "Epoch: 5 Batch: 10  Loss: 1.1671267747879028\n",
      "Epoch: 5 Batch: 11  Loss: 1.548757791519165\n",
      "Epoch: 5 Batch: 12  Loss: 1.6769113540649414\n",
      "Epoch: 5 Batch: 13  Loss: 1.9663889408111572\n",
      "Epoch: 5 Batch: 14  Loss: 1.3873670101165771\n",
      "Epoch: 5 Batch: 15  Loss: 1.2138886451721191\n",
      "Epoch: 5 Batch: 16  Loss: 1.5619615316390991\n",
      "Epoch: 5 Batch: 17  Loss: 1.2273157835006714\n",
      "Epoch: 5 Batch: 18  Loss: 1.8360564708709717\n",
      "Epoch: 5 Batch: 19  Loss: 2.943058967590332\n",
      "Epoch: 5 Batch: 20  Loss: 1.4837148189544678\n",
      "Epoch: 5 Batch: 21  Loss: 0.7779262065887451\n",
      "Epoch: 5 Batch: 22  Loss: 1.226905107498169\n",
      "Epoch: 5 Batch: 23  Loss: 2.421757221221924\n",
      "Epoch: 5 Batch: 24  Loss: 1.310930848121643\n",
      "Epoch: 5 Batch: 25  Loss: 6.126866817474365\n",
      "Epoch: 5 Batch: 26  Loss: 1.5888431072235107\n",
      "Epoch: 5 Batch: 27  Loss: 1.296592354774475\n",
      "Epoch: 5 Batch: 28  Loss: 1.4401381015777588\n",
      "Epoch: 5 Batch: 29  Loss: 1.331925630569458\n",
      "Epoch: 5 Batch: 30  Loss: 6.844159126281738\n",
      "Epoch: 5 Batch: 31  Loss: 6.347686767578125\n",
      "Epoch: 5 Batch: 32  Loss: 2.6553597450256348\n",
      "Epoch: 5 Batch: 33  Loss: 1.8423848152160645\n",
      "Epoch: 5 Batch: 34  Loss: 1.2243198156356812\n",
      "Epoch: 5 Batch: 35  Loss: 0.9721986651420593\n",
      "Epoch: 5 Batch: 36  Loss: 2.1239383220672607\n",
      "Epoch: 5 Batch: 37  Loss: 9.460799217224121\n",
      "Epoch: 5 Batch: 38  Loss: 1.4356093406677246\n",
      "Epoch: 5 Batch: 39  Loss: 1.6827585697174072\n",
      "Epoch: 5 Batch: 40  Loss: 1.9586471319198608\n",
      "Epoch: 5 Batch: 41  Loss: 1.2665863037109375\n",
      "Epoch: 5 Batch: 42  Loss: 1.9443882703781128\n",
      "Epoch: 5 Batch: 43  Loss: 1.4169037342071533\n",
      "Epoch: 5 Batch: 44  Loss: 2.856801986694336\n",
      "Epoch: 5 Batch: 45  Loss: 1.0964560508728027\n",
      "Epoch: 5 Batch: 46  Loss: 1.672025442123413\n",
      "Epoch: 5 Batch: 47  Loss: 1.0347068309783936\n",
      "Epoch: 5 Batch: 48  Loss: 1.277628779411316\n",
      "Epoch: 5 Batch: 49  Loss: 4.124227523803711\n",
      "Epoch: 5 Batch: 50  Loss: 1.5387864112854004\n",
      "Epoch: 5 Batch: 51  Loss: 2.294304847717285\n",
      "Epoch: 5 Batch: 52  Loss: 1.0555686950683594\n",
      "Epoch: 5 Batch: 53  Loss: 6.624955654144287\n",
      "Epoch: 5 Batch: 54  Loss: 2.7598156929016113\n",
      "Epoch: 5 Batch: 55  Loss: 1.9087040424346924\n",
      "Epoch: 5 Batch: 56  Loss: 1.0636337995529175\n",
      "Epoch: 5 Batch: 57  Loss: 0.9829279780387878\n",
      "Epoch: 5 Batch: 58  Loss: 2.60778546333313\n",
      "Epoch: 5 Batch: 59  Loss: 1.7003142833709717\n",
      "Epoch: 5 Batch: 60  Loss: 2.3563477993011475\n",
      "Epoch: 5 Batch: 61  Loss: 6.847767353057861\n",
      "Epoch: 5 Batch: 62  Loss: 2.1627185344696045\n",
      "Epoch: 5 Batch: 63  Loss: 1.9538583755493164\n",
      "Epoch: 5 Batch: 64  Loss: 1.7442814111709595\n",
      "Epoch: 5 Batch: 65  Loss: 2.1118857860565186\n",
      "Epoch: 5 Batch: 66  Loss: 1.294891595840454\n",
      "Epoch: 5 Batch: 67  Loss: 1.936863660812378\n",
      "Epoch: 5 Batch: 68  Loss: 0.97948157787323\n",
      "Epoch: 5 Batch: 69  Loss: 3.403484582901001\n",
      "Epoch: 5 Batch: 70  Loss: 3.029672861099243\n",
      "Epoch: 5 Batch: 71  Loss: 1.5894660949707031\n",
      "Epoch: 5 Batch: 72  Loss: 1.343671202659607\n",
      "Epoch: 5 Batch: 73  Loss: 1.7827885150909424\n",
      "Epoch: 5 Batch: 74  Loss: 1.2243068218231201\n",
      "Epoch: 5 Batch: 75  Loss: 1.9226384162902832\n",
      "Epoch: 5 Batch: 76  Loss: 2.228153944015503\n",
      "Epoch: 5 Batch: 77  Loss: 1.8499867916107178\n",
      "Epoch: 5 Batch: 78  Loss: 1.471016526222229\n",
      "Epoch: 5 Batch: 79  Loss: 1.3703036308288574\n",
      "------------------------------------------------\n",
      "Epoch: 6 Batch: 0  Loss: 1.5203757286071777\n",
      "Epoch: 6 Batch: 1  Loss: 1.2356171607971191\n",
      "Epoch: 6 Batch: 2  Loss: 1.4710041284561157\n",
      "Epoch: 6 Batch: 3  Loss: 1.3145534992218018\n",
      "Epoch: 6 Batch: 4  Loss: 2.060422420501709\n",
      "Epoch: 6 Batch: 5  Loss: 1.9264588356018066\n",
      "Epoch: 6 Batch: 6  Loss: 1.2181117534637451\n",
      "Epoch: 6 Batch: 7  Loss: 1.6729735136032104\n",
      "Epoch: 6 Batch: 8  Loss: 1.016198754310608\n",
      "Epoch: 6 Batch: 9  Loss: 4.935141086578369\n",
      "Epoch: 6 Batch: 10  Loss: 1.5663151741027832\n",
      "Epoch: 6 Batch: 11  Loss: 1.469968318939209\n",
      "Epoch: 6 Batch: 12  Loss: 1.487386703491211\n",
      "Epoch: 6 Batch: 13  Loss: 2.7082462310791016\n",
      "Epoch: 6 Batch: 14  Loss: 2.0610005855560303\n",
      "Epoch: 6 Batch: 15  Loss: 1.1481300592422485\n",
      "Epoch: 6 Batch: 16  Loss: 1.651871681213379\n",
      "Epoch: 6 Batch: 17  Loss: 1.4597760438919067\n",
      "Epoch: 6 Batch: 18  Loss: 1.4087742567062378\n",
      "Epoch: 6 Batch: 19  Loss: 1.6557328701019287\n",
      "Epoch: 6 Batch: 20  Loss: 1.686408281326294\n",
      "Epoch: 6 Batch: 21  Loss: 2.159144878387451\n",
      "Epoch: 6 Batch: 22  Loss: 1.428715467453003\n",
      "Epoch: 6 Batch: 23  Loss: 1.959770679473877\n",
      "Epoch: 6 Batch: 24  Loss: 0.9458502531051636\n",
      "Epoch: 6 Batch: 25  Loss: 1.1722745895385742\n",
      "Epoch: 6 Batch: 26  Loss: 8.614383697509766\n",
      "Epoch: 6 Batch: 27  Loss: 1.7192416191101074\n",
      "Epoch: 6 Batch: 28  Loss: 1.2129343748092651\n",
      "Epoch: 6 Batch: 29  Loss: 1.7377026081085205\n",
      "Epoch: 6 Batch: 30  Loss: 7.053077220916748\n",
      "Epoch: 6 Batch: 31  Loss: 4.225348949432373\n",
      "Epoch: 6 Batch: 32  Loss: 1.2551774978637695\n",
      "Epoch: 6 Batch: 33  Loss: 3.444986343383789\n",
      "Epoch: 6 Batch: 34  Loss: 1.5473854541778564\n",
      "Epoch: 6 Batch: 35  Loss: 1.3390296697616577\n",
      "Epoch: 6 Batch: 36  Loss: 7.244062423706055\n",
      "Epoch: 6 Batch: 37  Loss: 1.7658880949020386\n",
      "Epoch: 6 Batch: 38  Loss: 1.745771884918213\n",
      "Epoch: 6 Batch: 39  Loss: 2.137885332107544\n",
      "Epoch: 6 Batch: 40  Loss: 4.956882953643799\n",
      "Epoch: 6 Batch: 41  Loss: 1.3402397632598877\n",
      "Epoch: 6 Batch: 42  Loss: 1.4777859449386597\n",
      "Epoch: 6 Batch: 43  Loss: 1.7707507610321045\n",
      "Epoch: 6 Batch: 44  Loss: 2.5769176483154297\n",
      "Epoch: 6 Batch: 45  Loss: 2.7569079399108887\n",
      "Epoch: 6 Batch: 46  Loss: 2.222547769546509\n",
      "Epoch: 6 Batch: 47  Loss: 2.287590980529785\n",
      "Epoch: 6 Batch: 48  Loss: 1.3785446882247925\n",
      "Epoch: 6 Batch: 49  Loss: 3.7144711017608643\n",
      "Epoch: 6 Batch: 50  Loss: 1.600733757019043\n",
      "Epoch: 6 Batch: 51  Loss: 0.9229518175125122\n",
      "Epoch: 6 Batch: 52  Loss: 1.1948721408843994\n",
      "Epoch: 6 Batch: 53  Loss: 0.8910067677497864\n",
      "Epoch: 6 Batch: 54  Loss: 0.8405701518058777\n",
      "Epoch: 6 Batch: 55  Loss: 1.1939195394515991\n",
      "Epoch: 6 Batch: 56  Loss: 0.8482509255409241\n",
      "Epoch: 6 Batch: 57  Loss: 0.9928035736083984\n",
      "Epoch: 6 Batch: 58  Loss: 1.6115455627441406\n",
      "Epoch: 6 Batch: 59  Loss: 1.5916476249694824\n",
      "Epoch: 6 Batch: 60  Loss: 0.8079317808151245\n",
      "Epoch: 6 Batch: 61  Loss: 0.9406389594078064\n",
      "Epoch: 6 Batch: 62  Loss: 4.370591163635254\n",
      "Epoch: 6 Batch: 63  Loss: 1.7153609991073608\n",
      "Epoch: 6 Batch: 64  Loss: 3.744692325592041\n",
      "Epoch: 6 Batch: 65  Loss: 1.081498622894287\n",
      "Epoch: 6 Batch: 66  Loss: 2.131817579269409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Batch: 67  Loss: 1.833629846572876\n",
      "Epoch: 6 Batch: 68  Loss: 1.6958562135696411\n",
      "Epoch: 6 Batch: 69  Loss: 1.7112674713134766\n",
      "Epoch: 6 Batch: 70  Loss: 5.45814847946167\n",
      "Epoch: 6 Batch: 71  Loss: 1.7969809770584106\n",
      "Epoch: 6 Batch: 72  Loss: 1.6901531219482422\n",
      "Epoch: 6 Batch: 73  Loss: 1.026635766029358\n",
      "Epoch: 6 Batch: 74  Loss: 1.5828747749328613\n",
      "Epoch: 6 Batch: 75  Loss: 4.64332389831543\n",
      "Epoch: 6 Batch: 76  Loss: 4.980201244354248\n",
      "Epoch: 6 Batch: 77  Loss: 1.624232530593872\n",
      "Epoch: 6 Batch: 78  Loss: 1.8197154998779297\n",
      "Epoch: 6 Batch: 79  Loss: 1.4158737659454346\n",
      "------------------------------------------------\n",
      "Epoch: 7 Batch: 0  Loss: 5.391907691955566\n",
      "Epoch: 7 Batch: 1  Loss: 2.484057903289795\n",
      "Epoch: 7 Batch: 2  Loss: 1.722579836845398\n",
      "Epoch: 7 Batch: 3  Loss: 1.500098466873169\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mD:\\SYSU\\ObjDet\\MyProject\\Faster_RCNN\\train.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgt_with_cls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mim_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mim_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgt_with_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xjm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\SYSU\\ObjDet\\MyProject\\Faster_RCNN\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img, im_info, gts_with_cls)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mfeat_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeat_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#print(feat_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mfeat_pool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrpn_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpositive_keep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnegative_keep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcls_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreg_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mim_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgts_with_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;31m#print(rpn_loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mfeat_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeat_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xjm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\SYSU\\ObjDet\\MyProject\\Faster_RCNN\\rpn_module\\rpn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, feat_map, im_info, gt_boxes_with_cls)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mrpn_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAnchor_Target_Layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrpn_score_reshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrpn_bbox_reshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manchors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgt_boxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# Give Cls-Reg Targets Here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mpositive_keep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnegative_keep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcls_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreg_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProposal_Target_Layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrois\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgt_boxes_with_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfeat_pool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrpn_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpositive_keep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnegative_keep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcls_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreg_target\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xjm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\SYSU\\ObjDet\\MyProject\\Faster_RCNN\\rpn_module\\proposal_target_layer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, proposals, gts_with_cls, cls_num, max_sample_num, neg_thr, pos_thr, np_rate)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproposal\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# if not in positive keep just classify it as negative_upper_bound\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mPositive_Keep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m                 \u001b[0mcls_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgts_with_cls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manchor_max_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0msingle_reg_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msingle_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproposal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgts_with_cls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manchor_max_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xjm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__contains__\u001b[1;34m(self, element)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \"\"\"\n\u001b[0;32m    505\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m         raise RuntimeError(\n",
      "\u001b[1;32mc:\\users\\xjm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
